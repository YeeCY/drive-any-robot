project_name: stable_contrastive_rl
run_name: stable_contrastive_rl_debug

# training setup
use_wandb: True  # set to false if you don't want to log wandb
train: rl
batch_size: 16  # default = 16, full size = 207
eval_batch_size: 16  # default = 400
epochs: 120
gpu_ids: [0, 1, 2, 3] # list of gpu ids to use
num_workers: 16
lr: 5e-4
optimizer: adam
seed: 0

# model params
model_type: stable_contrastive_rl
obs_encoding_size: 1024
goal_encoding_size: 1024
twin_q: True
min_log_std: -13
max_log_std: -2
fixed_std: [0.1, 0.1, 0.1, 0.1, 0.1,
            0.1, 0.1, 0.1, 0.1, 0.1,
            0.1, 0.1, 0.1, 0.1, 0.1,
            0.1, 0.1, 0.1, 0.1, 0.1,
            0.5]
soft_target_tau: 0.005

# normalization for the action space
normalize: True

# context
context_type: temporal
context_size: 5

# discount factor
discount: 0.99

# whether to use C-Learning (TD) or Contrastive NCE (MC)
use_td: False

# GCBC
bc_coef: 0.5
mle_gcbc_loss: True

# whether to stop gradients from actor loss to policy image encoder
stop_grad_actor_img_encoder: True
use_actor_waypoint_q_loss: True
use_actor_dist_q_loss: False

# number of training iterations to update target q network
target_update_freq: 1

# distance bounds for distance and action and distance predictions
rl:
  min_dist_cat: 2
  max_dist_cat: 12
  oracle_angles: 45  # degree
  num_oracle_trajs: 10
close_far_threshold: 6  # distance threshold used to seperate the close and the far subgoals that are sampled per datapoint

# action output params
len_traj_pred: 5
learn_angle: True

# dataset specific parameters
image_size: [85, 64] # width, height
datasets:
  recon:
    data_folder: datasets/recon_subset # path to the dataset
    train: gnm_train/data/data_splits/recon_subset/train/ # path to the train dir with traj_names.txt
    test: gnm_train/data/data_splits/recon_subset/test/ # path to the test dir with traj_names.txt
    end_slack: 3 # because many trajectories end in collisions
    goals_per_obs: 1 # how many goals are sampled per observation
#    negative_mining: True # negative mining from the ViNG paper (Shah et al.)
    discount: 0.99  # discount factor used to sample geometric future goals, can be a negative value for random future goals

# logging stuff
print_log_freq: 100 # in iterations
image_log_freq: 1000 # in iterations
num_images_log: 16 # number of images to log in a logging iteration
pairwise_test_freq: 10 # in epochs
